{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "030b4cee",
   "metadata": {},
   "source": [
    "### Preparación datos modelo\n",
    "\n",
    "Preparar los datos para los modelos de inferencia.\n",
    "\n",
    "Necesita mergear:\n",
    "* Archivo con los tweets y los stats para cada uno de ellos (solo nos interesan lo de engagement).\n",
    "    * Dejar la fecha solo como año/mes/dia.\n",
    "    * Eliminar RTs, el resto nos los podemos quedar.\n",
    "* Archivo con las dimensiones para cada uno.\n",
    "    * Sería una columna por dimensión. Dimensión original si se está usando el modelo de Aiello.\n",
    "\n",
    "Hacemos primero el filtrado de los municipios que nos interesan (de acuerdo a la cantidad de tweets en cada período).\n",
    "\n",
    "* Normaliza con percentil y de acuerdo al ``word_length``.\n",
    "* Summariza de acuerdo a tweet/user_id y luego por distintos períodos temporales none/day/week/month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715b137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade7ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# las fechas y el df_stats para filtrar los que nos interesan\n",
    "from datetime import datetime\n",
    "\n",
    "data_dir = './df_merged/'\n",
    "inic_no_covid = datetime(2019,1,1)\n",
    "no_covid = datetime(2020,1,1)\n",
    "pre_covid = datetime(2020,3,1)\n",
    "post_covid = datetime(2022,1,1)\n",
    "\n",
    "df_stats = pd.read_pickle(data_dir + 'df_date_stats.pickle')\n",
    "df_stats = df_stats[(df_stats['no_covid'] > 0) & (df_stats['pre_covid'] > 0) & (df_stats['during_covid'] > 0) & (df_stats['post_covid'] > 0)]\n",
    "len(df_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e1013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.read_pickle(data_dir + 'df_tweets_db.pickle')\n",
    "if df_tweets['created_at'].values[0] is datetime:\n",
    "    df_tweets['created_at'] = df_tweets['created_at'].dt.date\n",
    "df_tweets = df_tweets[df_tweets['retweeted_id'] == -1]\n",
    "df_tweets = df_tweets[df_tweets['user_id'].isin(df_stats.index)]\n",
    "len(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56987610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos las dimensiones para trabajar\n",
    "ten_dims = ['knowledge','power','status','trust','support','romance','similarity','identity','fun','conflict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f657da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mergeamos la info de las cuentas de usuario (variables de control según los papers)\n",
    "df_user_stats = pd.read_pickle(data_dir + 'df_user_current_stats.pickle')\n",
    "df_user_stats\n",
    "\n",
    "df_tweets = df_tweets.reset_index().merge(df_user_stats,left_on='user_id',right_on='user_id').set_index('tweet_id')\n",
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb078711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agregamos columnas que podrían ser de control para los tweets\n",
    "df_tweets['hashtag_count'] = df_tweets['hashtags'].map(len)\n",
    "df_tweets['media_count'] = df_tweets['media'].map(len)\n",
    "df_tweets['mentions_count'] = df_tweets['mentions'].map(len)\n",
    "df_tweets['url_count'] = df_tweets['url'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bcdda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nos quedamos solo con las columnas que son de interés\n",
    "df_tweets_reduced = df_tweets[['created_at','user_id','favorite_count','retweet_count','reply_count','word_length','sentence_length','hashtag_count','media_count','mentions_count','url_count','followers','followees','listed','tweets']]\n",
    "df_tweets_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ae256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column tweets_before -- como sacamos la hora de los tweets, vamos a contar hasta el día anterior de cada uno\n",
    "# esto hace que tweets del mismo día tengan la misma cantidad de tweets previos de ese usuario\n",
    "import os\n",
    "\n",
    "if os.path.exists(data_dir + 'df_tweets_reduced.pickle'):\n",
    "    dd = pd.read_pickle(data_dir + 'df_tweets_reduced.pickle')\n",
    "    df_tweets_reduced = df_tweets_reduced.merge(dd[['tweets_before']],left_index=True,right_index=True)\n",
    "else:\n",
    "    dd = deque()\n",
    "    user_dates = {}\n",
    "    for i in tqdm(range(0,len(df_tweets_reduced))):\n",
    "        uu = df_tweets_reduced['user_id'].values[i]\n",
    "        if uu not in user_dates: # al menos que no tenga que recalcular los tweets de un usuario en particular, siendo que hay solo 80 usuarios \n",
    "            user_dates[uu] = list(df_tweets_reduced[df_tweets_reduced['user_id'] == uu]['created_at'].values)\n",
    "\n",
    "        d = df_tweets_reduced['created_at'].values[i]\n",
    "        dd.append(sum(1 for x in user_dates[uu] if x < d))\n",
    "\n",
    "    df_tweets_reduced['tweets_before'] = dd\n",
    "\n",
    "    df_tweets_reduced.to_pickle(data_dir + 'df_tweets_reduced.pickle')\n",
    "\n",
    "df_tweets_reduced.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d6c49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'df_tweets_social_dimensions_empath_median-no-zeros.pickle' # TODO: Acá se cambia el archivo\n",
    "file_name = 'df_tweets_social_dimensions_model_simplified.pickle'\n",
    "\n",
    "df_dimensions = pd.read_pickle(data_dir + file_name) \n",
    "\n",
    "df_merged = pd.merge(df_tweets_reduced, df_dimensions,left_index=True,right_index=True)\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcecc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115ec6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en cualquiera de los casos, se hace el promedio o la media para agrupar\n",
    "# primero se hace la división en tiempo y luego en lo que sea\n",
    "\n",
    "# time_unit: tweet, day, week, month, phase\n",
    "# agg_muni: None, user_id\n",
    "def get_representation(df,time_unit=None,agg_muno='tweet',agg_func='median'): \n",
    "    \n",
    "    df_agg = None\n",
    "    if agg_muno == 'tweet': # la unidad base va a ser el tweet\n",
    "        df_agg = pd.DataFrame(df.to_dict())\n",
    "#         df_agg = df_agg.drop(columns=['user_id'])\n",
    "    else:\n",
    "        df_agg = df.groupby(by=['user_id','created_at']).agg(agg_func).reset_index().set_index('user_id')\n",
    "        \n",
    "    if time_unit is None: # row: tweet, columns: features\n",
    "#         if 'created_at' in df_agg.columns:\n",
    "#             df_agg = df_agg.drop(columns=['created_at'])\n",
    "        return df_agg\n",
    "    elif time_unit == 'day': # row: day, columns: features\n",
    "        df_agg = df_agg.groupby(by='created_at').agg(agg_func) \n",
    "        return df_agg\n",
    "    elif time_unit == 'week': # row: week, columns: features\n",
    "        df_agg['created_at'] = pd.to_datetime(df_agg['created_at']).dt.to_period('W')\n",
    "        df_agg = df_agg.groupby(by='created_at').agg(agg_func) \n",
    "        return df_agg\n",
    "    elif time_unit == 'month': #row: month, columns: features\n",
    "        df_agg['created_at'] = pd.to_datetime(df_agg['created_at']).dt.to_period('M')\n",
    "        df_agg = df_agg.groupby(by='created_at').agg(agg_func) \n",
    "        return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a71ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# este se puede usar todo junto, o dividido por los tramos y a cada uno se le aplica la transformación \n",
    "# y se lo guarda listo para ser usado, presumiblemente en otra notebook\n",
    "# get_representation(df_merged,agg_muno='user_id',time_unit='week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a8d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribución para determinar si tiene o no tiene el trait\n",
    "# se puede aplicar antes o después de obtener la representación, con lo que pueden ser valores puros o valores promediados\n",
    "\n",
    "def normalize_scores(df,columns, score_norm=None, perc=0.85, words_norm=None):\n",
    "    df_norm = pd.DataFrame(df.to_dict())\n",
    "    \n",
    "    if score_norm == 'perc': # todo aquello que tenga un valor por encima del perc pasado por parámetro es 1, el resto 0\n",
    "        df_norm[columns] = (df_norm[columns] > df_norm[columns].quantile(perc)).astype('int')\n",
    "    \n",
    "    if words_norm == 'length_discount': # este es columna a columna porque los valores incluidos en la standararización varían\n",
    "        for c in columns: # por cada una de las columnas\n",
    "            mean = df_norm[df_norm[c] > 0]['word_length'].mean()\n",
    "            std = df_norm[df_norm[c] > 0]['word_length'].std()\n",
    "            vv = deque()\n",
    "            for i in range(0, len(df_norm)):\n",
    "                ss = df_norm[c].values[i]\n",
    "                if ss == 0:\n",
    "                    vv.append(0)\n",
    "                else:\n",
    "                    zscore = (df_norm['word_length'].values[i] - mean) / std\n",
    "                    if zscore > 0:\n",
    "                        vv.append(ss * (1 / (1 + zscore))) # si está normalizado, multiplica por 1, sino por el score real\n",
    "                    else:\n",
    "                        vv.append(ss * (2 - (1 / (1 - zscore))))\n",
    "                    \n",
    "            df_norm[c] = vv\n",
    "    \n",
    "    return df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e0bfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm = normalize_scores(df_merged,ten_dims,score_norm='perc')\n",
    "df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f6cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ww = 'word_length'\n",
    "# ww = 'sentence_length'\n",
    "for x in ten_dims:\n",
    "    print(x,' yes -- ',df_norm[df_norm[x] > 0][[ww]].describe().to_dict())\n",
    "    print(x,' no -- ',df_norm[df_norm[x] == 0][[ww]].describe().to_dict())\n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b6538",
   "metadata": {},
   "source": [
    "#### Generación de los archivos de entrada del modelo\n",
    "\n",
    "El input base para todo es ``df_merged``.\n",
    "\n",
    "Generar archivos base:\n",
    "•\tCon o sin normalización\n",
    "•\tCon o sin length standarizacion\n",
    "•\tTodos juntos o divididos por fecha (lo de normalizar debería hacerse después de esto)\n",
    "•\tCon diferentes granularidades, tweet/user, None/day/week/month\n",
    "\n",
    "1.\tTodo junto o por grupo de fechas.\n",
    "2.\tNormalización + standarización\n",
    "3.\tGranularidad\n",
    "\n",
    "No deberían ser muchos archivos los que se generen, porque tampoco queremos probar todo YA. Lo de agrupar por tweet/user tiene sentido que se haga al final.\n",
    "Lo malo de esto es que vamos a tener muchos promedios/promedios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1215af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variants(df,prefix,file_name):\n",
    "    agg = ['tweet','user_id']\n",
    "    time = [None,'day','week','month']\n",
    "\n",
    "    for aa in agg:\n",
    "        for tt in time:\n",
    "            print(prefix+aa+'_'+str(tt)+'__' + file_name)\n",
    "            df_summ = get_representation(df,agg_muno=aa,time_unit=tt)\n",
    "            print(len(df_summ))\n",
    "            df_summ.to_pickle(prefix+aa+'_'+str(tt)+'__' + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed4dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sin separar por fecha -- normaliza todo junto y luego hace las agregaciones correspondientes\n",
    "df_norm = normalize_scores(df_merged,ten_dims,score_norm='perc')\n",
    "get_variants(df_norm,'df_merged__all__normalized_perc_85__',file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d909996",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm = normalize_scores(df_merged,ten_dims,score_norm=None)\n",
    "get_variants(df_norm,'df_merged__all__',file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b625f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sin separar por fecha -- hace las dos normalizaciones todo junto y luego hace las agregaciones correspondientes\n",
    "df_norm = normalize_scores(df_merged,ten_dims,score_norm='perc',words_norm='length_discount')\n",
    "get_variants(df_norm,'df_merged__all__normalized_perc_85_word_length__',file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46f1e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividimos en los grupos por fecha, por cada uno de los grupos, hacemos lo de antes\n",
    "# estos se podrían separar de los otros y que luego solo haya que hacer el merge que corresponda\n",
    "\n",
    "df_merged_no_covid = df_merged[(df_merged['created_at'] <= no_covid.date())]\n",
    "print(len(df_merged_no_covid))\n",
    "\n",
    "df_merged_pre_covid = df_merged[(df_merged['created_at'] > no_covid.date()) & (df_merged['created_at'] <= pre_covid.date())]\n",
    "print(len(df_merged_pre_covid))\n",
    "\n",
    "df_merged_during_covid = df_merged[(df_merged['created_at'] > pre_covid.date()) & (df_merged['created_at'] <= post_covid.date())]\n",
    "print(len(df_merged_during_covid))\n",
    "\n",
    "df_merged_post_covid = df_merged[(df_merged['created_at'] > post_covid.date())]\n",
    "print(len(df_merged_post_covid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99805add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_norm = normalize_scores(df_merged_no_covid,ten_dims,score_norm=None)\n",
    "# get_variants(df_norm,'df_merged__no_covid__',file_name)\n",
    "\n",
    "# df_norm = normalize_scores(df_merged_no_covid,ten_dims,score_norm='perc')\n",
    "# get_variants(df_norm,'df_merged__no_covid__normalized_perc_85__',file_name)\n",
    "\n",
    "# df_norm = normalize_scores(df_merged_no_covid,ten_dims,score_norm='perc',words_norm='length_discount')\n",
    "# get_variants(df_norm,'df_merged__no_covid__normalized_perc_85_word_length__',file_name)\n",
    "\n",
    "df_norm = normalize_scores(df_merged_pre_covid,ten_dims,score_norm=None)\n",
    "print(len(df_norm))\n",
    "get_variants(df_norm,'df_merged__pre_covid__',file_name)\n",
    "\n",
    "# df_norm = normalize_scores(df_merged_pre_covid,ten_dims,score_norm='perc')\n",
    "# get_variants(df_norm,'df_merged__pre_covid__normalized_perc_85__',file_name)\n",
    "\n",
    "# df_norm = normalize_scores(df_merged_pre_covid,ten_dims,score_norm='perc',words_norm='length_discount')\n",
    "# get_variants(df_norm,'df_merged__pre_covid__normalized_perc_85_word_length__',file_name)\n",
    "\n",
    "df_norm = normalize_scores(df_merged_during_covid,ten_dims,score_norm=None)\n",
    "print(len(df_norm))\n",
    "get_variants(df_norm,'df_merged__during_covid__',file_name)\n",
    "\n",
    "# df_norm = normalize_scores(df_merged_during_covid,ten_dims,score_norm='perc')\n",
    "# get_variants(df_norm,'df_merged__during_covid__normalized_perc_85__',file_name)\n",
    "\n",
    "# df_norm = normalize_scores(df_merged_during_covid,ten_dims,score_norm='perc',words_norm='length_discount')\n",
    "# get_variants(df_norm,'df_merged__during_covid__normalized_perc_85_word_length__',file_name)\n",
    "\n",
    "df_norm = normalize_scores(df_merged_post_covid,ten_dims,score_norm=None)\n",
    "print(len(df_norm))\n",
    "get_variants(df_norm,'df_merged__post_covid__',file_name)\n",
    "\n",
    "# df_norm = normalize_scores(df_merged_post_covid,ten_dims,score_norm='perc')\n",
    "# get_variants(df_norm,'df_merged__post_covid__normalized_perc_85__',file_name)\n",
    "\n",
    "# df_norm = normalize_scores(df_merged_post_covid,ten_dims,score_norm='perc',words_norm='length_discount')\n",
    "# get_variants(df_norm,'df_merged__post_covid__normalized_perc_85_word_length__',file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9714b881",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_merged_no_covid_2019 = df_merged[(df_merged['created_at'] >= inic_no_covid.date()) & (df_merged['created_at'] <= no_covid.date())]\n",
    "# print(len(df_merged_no_covid_2019))\n",
    "# get_variants(df_merged_no_covid_2019,'df_merged__no_covid_2019__',file_name)\n",
    "\n",
    "df_norm = normalize_scores(df_merged_no_covid_2019,ten_dims,score_norm='perc')\n",
    "print(len(df_norm))\n",
    "get_variants(df_norm,'df_merged__no_covid_2019__normalized_perc_85__',file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4471dc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_no_covid_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a358cd57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
