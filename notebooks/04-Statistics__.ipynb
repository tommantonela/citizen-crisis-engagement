{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "066ff740",
   "metadata": {},
   "source": [
    "### Un poco más de estadística\n",
    "\n",
    "* Chequear diferencias estadísticas entre las dimensiones.\n",
    "    * En un mismo período y la misma across períodos.\n",
    "    * Antes/después de normalizar.\n",
    "* Chart de los traits.\n",
    "    * Lineal chart.\n",
    "    * Heatmap normalizado por trait.\n",
    "\n",
    "\n",
    "* Normality test para lo del modelo.\n",
    "\n",
    "\n",
    "Tests específicos para series de tiempo:\n",
    "\n",
    "* Chequear series de tiempo con Friedman (un análisis multi de diferencias estadísticas).\n",
    "* Chequear auto-correlación.\n",
    "* Chequear si son estacionarias o no.\n",
    "* No serial correlation.\n",
    "* Durbin Watson\n",
    "* Granger causality (efectos de grupo)\n",
    "* Similarity de series de tiempo (para evaluar across phases)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480cf463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff36fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.stats import wilcoxon\n",
    "from scipy.stats import brunnermunzel\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from statistics import mean, stdev\n",
    "from math import sqrt\n",
    "\n",
    "def cohens_d(c0,c1):\n",
    "    cohens_d = abs((mean(c0) - mean(c1)) / (sqrt((stdev(c0) ** 2 + stdev(c1) ** 2) / 2)))\n",
    "    if cohens_d < 0.1:\n",
    "        return 'negligent'\n",
    "    if cohens_d < 0.35:\n",
    "        return 'small' \n",
    "    if cohens_d < 0.65:\n",
    "        return 'medium'\n",
    "    return 'large'\n",
    "\n",
    "def unpaired_analysis(df,cols,alpha=0.01): # le pasamos un único df y calcula para todos los pares de columnas\n",
    "    differents = deque()\n",
    "    for i in range(0,len(cols)):\n",
    "        for j in range(i+1,len(cols)):\n",
    "            if scipy.stats.normaltest(df[cols[i]],nan_policy='omit').pvalue < alpha or scipy.stats.normaltest(df[cols[j]],nan_policy='omit').pvalue < alpha:\n",
    "                statsb = mannwhitneyu(df[cols[i]].dropna(),df[cols[j]].dropna(),alternative='two-sided')\n",
    "                statsg = mannwhitneyu(df[cols[i]].dropna(),df[cols[j]].dropna(),alternative='greater')\n",
    "                statsl = mannwhitneyu(df[cols[i]].dropna(),df[cols[j]].dropna(),alternative='less')\n",
    "                differents.append([cols[i],cols[j],statsb.pvalue,statsg.pvalue,statsl.pvalue,cohens_d(df[cols[i]].dropna(),df[cols[j]].dropna())])\n",
    "            else: # both normal\n",
    "                statsb = scipy.stats.ttest_ind(df[cols[i]],df[cols[j]],nan_policy='omit') \n",
    "                differents.append([cols[i],cols[j],statsb.statistic,statsb.pvalue,cohens_d(df[cols[i]].dropna(),df[cols[j]].dropna())])\n",
    "    return differents\n",
    "                \n",
    "def unpaired_analysis_across(df,df1,cols,alpha=0.01): # le pasamos un único df y calcula para todos los pares de columnas\n",
    "    differents = deque()\n",
    "    for i in tqdm(range(0,len(cols))):\n",
    "        \n",
    "            if list(df[cols[i]].values) == list(df1[cols[i]].values):\n",
    "                continue\n",
    "        \n",
    "            if scipy.stats.normaltest(df[cols[i]],nan_policy='omit').pvalue < alpha or scipy.stats.normaltest(df1[cols[i]],nan_policy='omit').pvalue < alpha:\n",
    "                statsb = mannwhitneyu(df[cols[i]].dropna(),df1[cols[i]].dropna(),alternative='two-sided')\n",
    "                statsg = mannwhitneyu(df[cols[i]].dropna(),df1[cols[i]].dropna(),alternative='greater')\n",
    "                statsl = mannwhitneyu(df[cols[i]].dropna(),df1[cols[i]].dropna(),alternative='less')\n",
    "                differents.append([cols[i],cols[i],statsb.pvalue,statsg.pvalue,statsl.pvalue,cohens_d(df[cols[i]].dropna(),df1[cols[i]].dropna())])\n",
    "            else: # both normal\n",
    "                statsb = scipy.stats.ttest_ind(df[cols[i]],df1[cols[i]],nan_policy='omit') \n",
    "                differents.append([cols[i],cols[i],statsb.statistic,statsb.pvalue,cohens_d(df[cols[i]].dropna(),df1[cols[i]].dropna())])\n",
    "    return differents\n",
    "      \n",
    "def unpaired_analysis_bm(df,df1,cols,alpha=0.01): # le pasamos un único df y calcula para todos los pares de columnas\n",
    "    differents = deque()\n",
    "    for i in tqdm(range(0,len(cols))):\n",
    "            if scipy.stats.normaltest(df[cols[i]],nan_policy='omit').pvalue < alpha or scipy.stats.normaltest(df1[cols[i]],nan_policy='omit').pvalue < alpha:\n",
    "                statsb = brunnermunzel(df[cols[i]].dropna(),df1[cols[i]].dropna(),alternative='two-sided')\n",
    "                statsg = brunnermunzel(df[cols[i]].dropna(),df1[cols[i]].dropna(),alternative='greater')\n",
    "                statsl = brunnermunzel(df[cols[i]].dropna(),df1[cols[i]].dropna(),alternative='less')\n",
    "                differents.append([cols[i],cols[i],statsb.pvalue,statsg.pvalue,statsl.pvalue,cohens_d(df[cols[i]].dropna(),df1[cols[i]].dropna())])\n",
    "            else: # both normal\n",
    "                statsb = scipy.stats.ttest_ind(df[cols[i]],df1[cols[i]],nan_policy='omit') \n",
    "                differents.append([cols[i],cols[i],statsb.statistic,statsb.pvalue,cohens_d(df[cols[i]].dropna(),df1[cols[i]].dropna())])\n",
    "    return differents\n",
    "    \n",
    "def paired_analysis(df,df1,cols,alpha=0.01): # pasamos dos dfs y calcula para la misma columna en los dos dfs\n",
    "    differents = deque()\n",
    "    colsdf1 = set(df1.columns)\n",
    "    for i in range(0,len(cols)):   \n",
    "        \n",
    "        print(cols[i], cols[i] in colsdf1)\n",
    "        \n",
    "        if cols[i] not in colsdf1:\n",
    "            continue\n",
    "        if len(df[cols[i]].dropna()) != len(df1[cols[i]].dropna()):\n",
    "            continue\n",
    "        if scipy.stats.normaltest(df[cols[i]],nan_policy='omit').pvalue < alpha or scipy.stats.normaltest(df1[cols[i]],nan_policy='omit').pvalue < alpha:\n",
    "            statsb = wilcoxon(df[cols[i]].dropna(),df1[cols[i]].dropna(),alternative='two-sided',)\n",
    "            statsg = wilcoxon(df[cols[i]].dropna(),df1[cols[i]].dropna(),alternative='greater')\n",
    "            statsl = wilcoxon(df[cols[i]].dropna(),df1[cols[i]].dropna(),alternative='less')\n",
    "            differents.append([cols[i],cols[i],statsb.pvalue,statsg.pvalue,statsl.pvalue,cohens_d(df[cols[i]].dropna(),df1[cols[i]].dropna())])\n",
    "        else: # both normal\n",
    "            statsb = scipy.stats.ttest_rel(df[cols[i]],df1[cols[i]],nan_policy='omit') \n",
    "            differents.append([cols[i],cols[i],statsb.statistic,statsb.pvalue,cohens_d(df[cols[i]].dropna(),df1[cols[i]].dropna())])\n",
    "    \n",
    "    return differents\n",
    "\n",
    "# compara para todas las columnas dentro de un mismo df, mismos datos, diferentes tratamientos\n",
    "def paired_analysis(df,cols,alpha=0.01): \n",
    "    \n",
    "    differents = deque()\n",
    "    for i in range(0,len(cols)):   \n",
    "        \n",
    "        for j in range(0,len(cols)):\n",
    "            \n",
    "            if i == j:\n",
    "                continue\n",
    "            \n",
    "            print(cols[i],cols[j])\n",
    "            \n",
    "            if list(df[cols[i]].values) == list(df[cols[j]].values):\n",
    "                continue\n",
    "            \n",
    "            if scipy.stats.normaltest(df[cols[i]],nan_policy='omit').pvalue < alpha or scipy.stats.normaltest(df[cols[j]],nan_policy='omit').pvalue < alpha:\n",
    "                statsb = wilcoxon(df[cols[i]],df[cols[j]],alternative='two-sided',)\n",
    "                statsg = wilcoxon(df[cols[i]],df[cols[j]],alternative='greater')\n",
    "                statsl = wilcoxon(df[cols[i]],df[cols[j]],alternative='less')\n",
    "                differents.append([cols[i],cols[j],statsb.pvalue,statsg.pvalue,statsl.pvalue,cohens_d(df[cols[i]].dropna(),df[cols[j]].dropna())])\n",
    "            else: # both normal\n",
    "                statsb = scipy.stats.ttest_rel(df[cols[i]],df[cols[j]],nan_policy='omit') \n",
    "                differents.append([cols[i],cols[j],statsb.statistic,statsb.pvalue,cohens_d(df[cols[i]].dropna(),df[cols[j]].dropna())])\n",
    "    \n",
    "    return differents\n",
    "\n",
    "def normality_test(df,cols=None,alpha=0.01):\n",
    "    if cols is None:\n",
    "        cols = df.columns\n",
    "    \n",
    "    norms = {}\n",
    "    for i in range(0,len(cols)): # hipótesis nula: la distribución es normal. pvalue < alpha se rechaza hipótesis nula\n",
    "        norms[cols[i]] = scipy.stats.normaltest(df[cols[i]],nan_policy='omit').pvalue\n",
    "    return norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b09cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# levantamos todos los dfs que nos interesan para una combinación de nombre\n",
    "\n",
    "def load_dfs(dir_data, name_part):\n",
    "    df_dict = {}\n",
    "\n",
    "#     df_dict['df_all'] = pd.read_pickle(dir_data + 'df_merged__all__' + name_part + '.pickle').sort_values(by='created_at')\n",
    "    \n",
    "    df_dict['df_no_covid_2019'] = pd.read_pickle(dir_data + 'df_merged__no_covid_2019__' + name_part + '.pickle').sort_values(by='created_at')\n",
    "    df_dict['df_no_covid'] = pd.read_pickle(dir_data + 'df_merged__no_covid__' + name_part + '.pickle').sort_values(by='created_at')\n",
    "    df_dict['df_pre_covid'] = pd.read_pickle(dir_data + 'df_merged__pre_covid__' + name_part + '.pickle').sort_values(by='created_at')\n",
    "    df_dict['df_during_covid'] = pd.read_pickle(dir_data + 'df_merged__during_covid__' + name_part + '.pickle').sort_values(by='created_at')\n",
    "    df_dict['df_post_covid'] = pd.read_pickle(dir_data + 'df_merged__post_covid__' + name_part + '.pickle').sort_values(by='created_at')\n",
    "\n",
    "    return df_dict\n",
    "\n",
    "dir_data = './df_merged/'\n",
    "name_part = 'normalized_perc_85__tweet_None__df_tweets_social_dimensions_model_simplified' # para facilitar generar los nombres\n",
    "name_part = 'normalized_perc_85__tweet_day__df_tweets_social_dimensions_model_simplified' \n",
    "# name_part = 'normalized_perc_85_word_length__tweet_None__df_tweets_social_dimensions_model_simplified'\n",
    "name_part = 'tweet_None__df_tweets_social_dimensions_model_simplified'\n",
    "# name_part = 'tweet_day__df_tweets_social_dimensions_model_simplified'\n",
    "# name_part = 'tweet_week__df_tweets_social_dimensions_model_simplified'\n",
    "\n",
    "df_dict = load_dfs(dir_data, name_part)\n",
    "for k,v in df_dict.items():\n",
    "    print(k,len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36188637",
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_dims = ['knowledge','power','status','trust','support','similarity','identity','fun','conflict'] # 'romance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728472d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,df in df_dict.items():\n",
    "    print(k, normality_test(df,ten_dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0fd35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dentro de un mismo grupo, calcular si hay diferencias\n",
    "listi = deque()\n",
    "\n",
    "for k,df in tqdm(df_dict.items()):\n",
    "    dicti = {}\n",
    "#     stats = paired_analysis(df,cols=ten_dims)\n",
    "    stats = paired_analysis(df,cols=['retweet_count','reply_count','favorite_count'])\n",
    "    print(stats)\n",
    "    for t in stats:\n",
    "        if len(t) < 6: # normal -- tiene que dividir y chequear para ver cual queremos rejectear\n",
    "            dicti[t[0]+'##'+t[1]] = t[4] if t[3]/2 < 0.01 and t[2] > 0 else '-'\n",
    "        else:\n",
    "            dicti[t[0]+'##'+t[1]] = t[5] if t[2] < 0.01 else '-'\n",
    "    dicti['df'] = k\n",
    "    listi.append(dicti)\n",
    "    \n",
    "df_significant = pd.DataFrame(listi).set_index('df').T\n",
    "# df_significant.to_csv('_df_paired_statistical_significances__'+name_part+'.csv')\n",
    "df_significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41aba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpaired analysis -- distintos datos, mismo tratamiento\n",
    "# cada fila es una combinación de dfs y cada columna es una dimensión\n",
    "listi = deque()\n",
    "ll = list(df_dict.items())\n",
    "for i in range(0,len(ll)):\n",
    "    for j in range(i+1,len(ll)):\n",
    "        print(ll[i][0] + '##' +  ll[j][0])\n",
    "        dicti = {}\n",
    "        stats = unpaired_analysis_across(ll[i][1],ll[j][1],cols=ten_dims)\n",
    "#         stats = unpaired_analysis_bm(ll[i][1],ll[j][1],cols=ten_dims)\n",
    "        print(stats)\n",
    "        for t in stats:\n",
    "            if len(t) < 6: # normal -- tiene que dividir y chequear para ver cual queremos rejectear\n",
    "                dicti[t[0]] = t[4] if t[3]/2 < 0.01 and t[2] > 0 else '-'\n",
    "            else:\n",
    "                dicti[t[0]] = t[5] if t[2] < 0.01 else '-'\n",
    "        dicti['df'] = ll[i][0] + '##' +  ll[j][0]  # necesita el nombre también!\n",
    "        listi.append(dicti)\n",
    "    \n",
    "df_significant = pd.DataFrame(listi).set_index('df').T\n",
    "df_significant.to_csv('_df_unpaired_statistical_significances_across_phases__'+name_part+'.csv')\n",
    "df_significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b798b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacer un line chart con el df pero agrupando por día\n",
    "# len(df[df[column] == 1]) / len(df[column]) # esto parece que haría lo del percentage\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def get_representation(df_data,dims,period='M',how='median',norm=None):\n",
    "    df = pd.DataFrame(df_data.to_dict())\n",
    "    df['created_at'] = pd.to_datetime(df['created_at']).dt.to_period(period)\n",
    "    if how != 'perc':\n",
    "        df = df.groupby(by='created_at')[dims].agg(how)\n",
    "    else:\n",
    "        df = df.groupby(by='created_at')[ten_dims]\n",
    "        df = df.sum() / df.count()\n",
    "    df = df.T\n",
    "    if norm is not None:\n",
    "        if norm == 'columns':\n",
    "            df = pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(df), columns=df.columns,index=df.index)\n",
    "        else:\n",
    "            df = pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(df.T), columns=df.T.columns,index=df.T.index).T\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f73f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in df_dict:\n",
    "    print('----- ',k)\n",
    "    dd = get_representation(df_dict[k],eng,period='D',how='sum',norm=None)\n",
    "    print(dd.T.corr().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e72918",
   "metadata": {},
   "source": [
    "* Similarity de series de tiempo (para evaluar across phases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558c0bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relación entre los grupos de dimensiones para una misma etapa\n",
    "\n",
    "from scipy.stats import kruskal\n",
    "from scipy.stats import friedmanchisquare\n",
    "\n",
    "\n",
    "for k in df_dict:\n",
    "    print('---',k)\n",
    "    print(friedmanchisquare(df_dict[k]['knowledge'],df_dict[k]['power'],df_dict[k]['status'],\n",
    "                 df_dict[k]['trust'],df_dict[k]['support'],df_dict[k]['identity'],\n",
    "                 df_dict[k]['fun'],df_dict[k]['conflict'],df_dict[k]['romance'],\n",
    "                 df_dict[k]['similarity']))\n",
    "    print(kruskal(df_dict[k]['knowledge'],df_dict[k]['power'],df_dict[k]['status'],\n",
    "                 df_dict[k]['trust'],df_dict[k]['support'],df_dict[k]['identity'],\n",
    "                 df_dict[k]['fun'],df_dict[k]['conflict'],df_dict[k]['romance'],\n",
    "                 df_dict[k]['similarity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41110c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dikey fuller test\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "adfuller(df_dict['df_all'][['knowledge']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a16ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# granger causality\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "df_ = df_dict['df_pre_covid']\n",
    "\n",
    "grangercausalitytests(df_[['knowledge', 'power']], maxlag=[1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9459a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict['df_no_covid']['knowledge'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea83943",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict['df_during_covid']['knowledge'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a993003",
   "metadata": {},
   "outputs": [],
   "source": [
    "_dims = ['power','status','trust','support','similarity','identity','fun','conflict']\n",
    "df_dict['df_post_covid'][ten_dims].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2123d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict['df_during_covid'][ten_dims].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57af70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict['df_pre_covid'][ten_dims].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8015a60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict['df_no_covid'][ten_dims].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a313de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df_dict['df_pre_covid'],df_dict['df_during_covid'],df_dict['df_post_covid']],axis=0)[['fun','similarity','identity']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d96fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict['df_no_covid'][ten_dims].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b70c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dikey fuller test\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "for d in ten_dims:\n",
    "    print(adfuller(df_dict['df_pre_covid'][[d]]))\n",
    "    \n",
    "from statsmodels.tsa.stattools import kpss\n",
    "kpss(df_dict['df_during_covid'][['trust']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8194a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# granger causality -- lo que me interesa ver son los casos donde una no me sirve para la otra, dado que ahí \"no habría relación\"\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "import numpy as np\n",
    "\n",
    "listi = []\n",
    "for k in df_dict:\n",
    "    df_ = df_dict[k].sort_values(by='created_at')\n",
    "    dicti = {}\n",
    "    print('-------------- ',k)\n",
    "    for i in tqdm(range(0,len(ten_dims))):\n",
    "        lag = kpss(df_[[ten_dims[i]]],nlags='legacy')[2] # 'legacy', 'auto' dan distintos resultados, MUY distintos\n",
    "        print(lag)\n",
    "        for j in range(0,len(ten_dims)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            dicti[ten_dims[i]+'##'+ten_dims[j]] = grangercausalitytests(df_[[ten_dims[i],ten_dims[j]]], maxlag=[lag],verbose=False)[lag][0]['params_ftest'][1]\n",
    "            print(ten_dims[i],'--',ten_dims[j],grangercausalitytests(df_[[ten_dims[i],ten_dims[j]]], maxlag=[lag],verbose=False)[lag][0]['params_ftest'])\n",
    "    dicti['df'] = k\n",
    "    listi.append(dicti)\n",
    "    \n",
    "aa = pd.DataFrame(listi)\n",
    "aa = aa.set_index('df').T\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d13df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.metrics import dtw\n",
    "# https://cs.stackexchange.com/questions/53250/normalized-measure-from-dynamic-time-warping\n",
    "# La similarity hay que normalizarla !!\n",
    "# para la similarity, necesitaría que la diferencia entre las mismas dimensiones across phases sea mayor que las internas?\n",
    "# si se da eso, es que son diferentes\n",
    "\n",
    "dtw_score = dtw(df_dict['df_pre_covid']['knowledge'], df_dict['df_post_covid']['knowledge'])\n",
    "dtw_score\n",
    "\n",
    "# para normalizar es (M(x) - D(x,y)) / M(x) y M(x) = len(x) * range(x) -- NO es tan claro esto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95920f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de granger\n",
    "name_part = 'tweet_None__df_tweets_social_dimensions_model_simplified'\n",
    "\n",
    "df_granger = pd.read_pickle('__df_granger__'+name_part+'.pickle')\n",
    "df_paired = pd.read_csv('_df_paired_statistical_significances__'+name_part+'.csv').rename(columns={'Unnamed: 0':'comb'}).set_index('comb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc01b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "\n",
    "ww = 'df_no_covid_2019'\n",
    "\n",
    "aa = df_granger[[ww]].merge(df_paired[[ww]],left_index=True,right_index=True,suffixes=('_granger','_paired'))\n",
    "aa[(aa[ww+'_granger'] > 0.01) & (aa[ww+'_paired'] != '-')]\n",
    "aa[(aa[ww+'_granger'] < 0.01)].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686f5359",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_ww = 'df_no_covid'\n",
    "ww = 'df_pre_covid'\n",
    "df_granger[(df_granger[pre_ww] < alpha) & (df_granger[ww] > alpha)][[pre_ww,ww]] # si -> no\n",
    "df_granger[(df_granger[pre_ww] < alpha) & (df_granger[ww] < alpha)][[pre_ww,ww]] # si -> si\n",
    "df_granger[(df_granger[pre_ww] > alpha) & (df_granger[ww] < alpha)][[pre_ww,ww]] # no -> si\n",
    "df_granger[(df_granger[pre_ww] < alpha) & (df_granger[ww] < alpha)][[pre_ww,ww]] # no -> no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865b0fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_ww = 'df_during_covid'\n",
    "ww = 'df_post_covid'\n",
    "df_paired[(df_paired[pre_ww] != '-') & (df_paired[ww] == '-')][[pre_ww,ww]] # si -> no\n",
    "df_paired[(df_paired[pre_ww] != '-') & (df_paired[ww] != '-')][[pre_ww,ww]] # si -> si\n",
    "df_paired[(df_paired[pre_ww] == '-') & (df_paired[ww] != '-')][[pre_ww,ww]] # no -> si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce2e806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
